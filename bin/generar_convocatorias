#!/usr/bin/env python3

import requests
import feedparser
import pdfplumber
import json
import re
import urllib.parse
from datetime import datetime
from pathlib import Path
import mysql.connector
import csv
from bs4 import BeautifulSoup

# ----------------------------
# CONFIGURACI√ìN
# ----------------------------
RSS_URL = "https://www.boe.es/rss/canal_per.php?l=p&c=140"
PDF_DIR = Path("pdfs")
PDF_DIR.mkdir(parents=True, exist_ok=True)
LOG_CSV = Path("python/boe/log_fallos.csv")

# Configuraci√≥n de la base de datos MySQL
db_config = {
    "host": "db",
    "user": "dev",
    "password": "dev",
    "database": "oposiciones"
}

# ------------------------------------------------
# 1) Clasificaci√≥n y mapeo de n√∫meros
# ------------------------------------------------

def classify_convocation(text):
    tl = text.lower()
    if any(k in tl for k in ["ayuntamiento de", "diputaci√≥n", "cabildo", "consell insular", "mancomunidad", "administraci√≥n local"]):
        return "municipal"
    elif any(k in tl for k in ["universidad de", "consorcio", "empresa p√∫blica"]):
        return "otros"
    elif any(k in tl for k in ["ministerio de", "subsecretar√≠a", "secretar√≠a de estado", "direcci√≥n general", "guardia civil", "polic√≠a nacional", "cuerpo de"]):
        return "estatal"
    else:
        return "otros"

num_map = {
    "cero": 0, "uno": 1, "una": 1, "dos": 2, "tres": 3, "cuatro": 4, "cinco": 5,
    # ‚Ä¶ completa hasta donde necesites ‚Ä¶
    "cien": 100
}

def word_to_number(w):
    return num_map.get(w, None)

# ------------------------------------------------
# 2) Funci√≥n principal de extracci√≥n
# ------------------------------------------------

def extract_fields(text):
    tipo = classify_convocation(text)
    organo = plazas = denominacion = localidad = plazo = ""
    warnings = []

    if tipo == "municipal":
        # ‚Ä¶ tu regex para municipal ‚Ä¶
        pass  # reemplaza con tu implementaci√≥n
    elif tipo == "estatal":
        # ‚Ä¶ tu regex para estatal ‚Ä¶
        pass
    else:
        # ‚Ä¶ tu regex para otros ‚Ä¶
        pass

    return {
        "tipo": tipo,
        "√≥rgano": organo,
        "n√∫mero de plazas": plazas,
        "denominaci√≥n": denominacion,
        "localidad/provincia": localidad,
        "plazo de solicitud": plazo,
        "warnings": warnings
    }

# ----------------------------
# FUNCIONES AUXILIARES
# ----------------------------

def extraer_texto_div_textoxslt(texto):
    """
   Extrae el texto dentro del div con id="textoxslt".
    Retorna el texto plano extra√≠do o cadena vac√≠a si no se encuentra o error.
    """
    try:
        soup = BeautifulSoup(texto, "html.parser")
        div = soup.find("div", id="textoxslt")
        if div:
            return div.get_text(separator="\n", strip=True)
        else:
            print(f"‚ö†Ô∏è No se encontr√≥ el div #textoxslt en texto.")
            return ""
    except Exception as e:
        print(f"‚ö†Ô∏è Error analizando texto: {e}")
        return ""


def descargar_pdf(url):
    """
    Descarga un PDF desde la URL si es un PDF v√°lido.
    """
    nombre_pdf = url.split("/")[-1]
    path = PDF_DIR / nombre_pdf
    if not path.exists():
        print(f"‚¨áÔ∏è Intentando descargar PDF: {nombre_pdf}")
        res = requests.get(url)
        if res.status_code == 200 and res.content.startswith(b"%PDF"):
            with open(path, "wb") as f:
                f.write(res.content)
        else:
            print(f"‚ö†Ô∏è PDF no disponible o inv√°lido: {url}")
            return None
    return path


def leer_pdf(path):
    """
    Lee un PDF con pdfplumber y devuelve su texto.
    """
    try:
        with pdfplumber.open(path) as pdf:
            return "\n".join(page.extract_text() or "" for page in pdf.pages)
    except Exception as e:
        print(f"‚ö†Ô∏è Error leyendo PDF {path.name}: {e}")
        return ""


def descargar_html(link, boe_id):
    """
    Intenta descargar el HTML con la fecha dada.
    """
    res = requests.get(link)
    texto = res.text if res.status_code == 200 else ""
    if res.status_code == 200 and "boe" in texto.lower() and "disposici√≥n" in texto.lower():
        texto = extraer_texto_div_textoxslt(texto)
        if texto.strip():
            print(f"üìÑ Usando TXT para {boe_id}")
            return texto
    print(f"‚ùå  TXT vac√≠o o no v√°lido para {boe_id}")
    return ""


def log_error(boe_id, motivo):
    """
    Registra errores en un CSV para revisarlos posteriormente.
    """
    if not LOG_CSV.exists():
        with open(LOG_CSV, mode="w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["boeId", "motivo", "timestamp"] )
    with open(LOG_CSV, mode="a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([boe_id, motivo, datetime.now().isoformat()])

# ----------------------------
# PROCESO PRINCIPAL
# ----------------------------

def main():
    print("üì° Leyendo feed RSS del BOE...")
    feed = feedparser.parse(RSS_URL)

    conn = mysql.connector.connect(**db_config)
    cursor = conn.cursor()

    for entry in feed.entries[0:20]:
        # 1) Extraer BOE ID desde entry.link (txt.php?id=...)
        m = re.search(r"id=(BOE-A-\d{4}-\d+)", entry.link)
        boe_id = m.group(1) if m else None

        if not boe_id:
            print(f"‚ö†Ô∏è No se pudo obtener el BOE ID para {entry.link}")
            log_error("N/A", "BOE ID no extra√≠do")
            continue

        # 2) Usar GUID para el PDF y pubDate para la fecha
        pdf_url = entry.guid
        fecha = entry.published_parsed

        # 3) Descargar y leer HTML
        texto = ""
        texto = descargar_html(entry.link, boe_id)

        # 4) Si falta contenido, fallback PDF
        if not texto.strip():
            pdf_path = descargar_pdf(pdf_url)
            if pdf_path:
                texto = leer_pdf(pdf_path)

        if not texto.strip():
            print(f"‚ö†Ô∏è {boe_id}: sin contenido legible en PDF ni HTML.")
            log_error(boe_id, "Sin PDF ni HTML disponible")
            continue

        # 5) Extraer campos y fecha de publicaci√≥n
        campos = extract_fields(texto)
        fecha_publi = datetime(*fecha[:6]).strftime("%Y-%m-%d %H:%M:%S") if fecha else datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # 6) Preparar datos para DB
        titulo = entry.title
        datos_extra = {
            "√≥rgano": campos['√≥rgano'],
            "plazas": campos['n√∫mero de plazas'],
            "localidad": campos['localidad/provincia'],
            "plazo": campos['plazo de solicitud'],
            "tipo": campos['tipo'],
            "warnings": campos['warnings']
        }
        for key in list(datos_extra):
            if not datos_extra[key]:
                del datos_extra[key]
            
            

        sql = """
        INSERT INTO CONVOCATORIA
        (boeId, titulo, texto, fechaPublicacion, enlace, datosExtra, createdAt, updatedAt)
        VALUES (%s, %s, %s, %s, %s, %s, NOW(), NOW())
        ON DUPLICATE KEY UPDATE
            titulo = VALUES(titulo),
            texto = VALUES(texto),
            fechaPublicacion = VALUES(fechaPublicacion),
            enlace = VALUES(enlace),
            datosExtra = VALUES(datosExtra),
            updatedAt = NOW()
        """

        cursor.execute(sql, (
            boe_id,
            titulo,
            texto,
            fecha_publi,
            entry.link,
            json.dumps(datos_extra, ensure_ascii=False)
        ))

        print(f"‚úÖ {boe_id} insertado correctamente.")

    conn.commit()
    cursor.close()
    conn.close()

if __name__ == "__main__":
    main()
